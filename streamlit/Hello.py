""" Oak AI AutoEval App.

The AutoEval tool evaluates lesson plans generated by the Oak AI Lesson 
Planning Assistant using a variety of lesson-plan-oriented metrics, 
mostly using a large language model (LLM) as a judge.

While LLMs can generate highly effective lesson plans, if we wish to 
improve their performance, we need to benchmark LLMs and prompts against 
each other in the context of our use case. By doing so, we can verify 
improvements in a data-driven way.

AutoEval enables users to:
- Upload lesson plans to the database
- Build new sample datasets of lesson plans for evaluation
- Construct new tests to run on a dataset
- Submit tests to be run on a dataset
- View the results of tests

Usage:

Run the following command in the terminal to open the AutoEval tool 
in a new tab in your default browser: `streamlit run Hello.py`

Streamlit's menu is located on the left sidebar of the screen. 
It is created automatically from the formatted file names of the Python 
scripts for each menu option in the pages/ directory. Click on a menu 
item to run the corresponding script in the main window.
"""

import streamlit as st

st.set_page_config(
    page_title="Hello",
    page_icon="ðŸ‘‹",
)

st.markdown(
    """ 
    # Welcome to Oak AI AutoEval App ðŸ‘‹
    
    **ðŸ‘ˆ Select a tab from the sidebar** to see what it can do!
    
    ***If your expected changes don't appear immediately*** try 
    clearing cache from the three dots on the top right corner of the 
    page /developer options and reload the page!
    
    ## What You Can Do With It:
    
    ### Upload Content
    - This page allows you to upload data into the lesson_plans table. 
    You can upload a csv file with a column containing your lesson 
    plans or other educational material. 
    
    ### Build Datasets
    - This page allows you to search-filter existing lesson plans 
    using key-stage, subject info and the generation_details of the 
    lesson_plans. You need to create a dataset to run evaluation 
    experiments. 

    ### Create Tests
    - You can review existing evaluation prompts and make changes to 
    create new prompts to test various aspects of your material using 
    AutoEval. It is also possible to review exactly what will be sent 
    to the LLM after rendering it with jinja.

    ### Run Auto Evaluation
    - You can select evaluation prompts, choose a dataset to run them 
    on, and start an experiment. The app will notify you when the 
    experiment has finished and can direct you to the insights page. 
    It also logs the status of the experiments. 
    
    ### Visualize Results
    - This interface allows you to explore detailed insights and data 
    regarding various educational experiments conducted. 

    **Your feedback is invaluable to us, so don't hesitate to share 
    your thoughts and report any issues you encounter!**
    """
)
